{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This notebook documents the progress of my passion project at Metis.\n",
    "# I'm including my learning notes, notes on setting up the environments, problem formulations, \n",
    "# as well as code samples for training an agent to play Atari games using Reinforcement Learning.\n",
    "\n",
    "# This can be used as supplemental material for optional lectures in week 9 and 10 of the bootcamp\n",
    "# Or a beginner's tutorial for final projects related to reinforcement learning.\n",
    "\n",
    "\n",
    "\n",
    "# Lingqiang Kong\n",
    "# 2017.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-77ba384a7a33>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-77ba384a7a33>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Understand Reinforcement Learning Problem setups and how it differs from supervised and unsupervised learning\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "* Understand Reinforcement Learning Problem setups and how it differs from supervised and unsupervised learning\n",
    "* Understand the Agent-Environment interface\n",
    "* Understand what MDPs (Markov Decision Processes) are and how to interpret transition diagrams\n",
    "* Understand Value Functions, Action-Value Functions, and Policy Functions\n",
    "* Understand the Bellman Equations and Bellman Optimality Equations for value functions and action-value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Nature: Learning by interacting with environments\n",
    "Learning what to do: How to map situations with actions, to get maximum reward\n",
    "- Learner action affects later input\n",
    "- Learner not told what action to take, must discover what actions yield the most reward by trying them out\n",
    "- Action may not just affect immediate reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences between Reinforcement Learning and Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Reinforcement learning is different from Supervise d Learning\n",
    "- There is no supervisor, only a reward signal (RL goals are to maximize the reward or Expectation of reward)\n",
    "- Feedback is delayed, not instantaenous\n",
    "- Agent's action affects subsequent inputs\n",
    "\n",
    "Supervised Learning\n",
    "- There is a \"correct\" label for each training sample\n",
    "- Time does not always matter (not always sequential)\n",
    "- Future inputs are not affected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "- Fly a stunt manuvour in a helicopter (that is very difficult for human to perform)\n",
    "- Beat world champion in a Chess (Go?) game\n",
    "- Play manu computer games better than human\n",
    "- Make a humanoid robot walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "At each time step t, the agent:\n",
    "    - Execute Action A_t\n",
    "    - Receives Oberservation O_t\n",
    "    - Receives Reward R_t \n",
    "\n",
    "t increments after each step\n",
    "\n",
    "We'll see this later in the OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A State S_t is considered **Markov** if and only if\n",
    "$$ P[S_t+1 | S_t] = P[S_{t+1} | S_1, S_2, ..., S_t]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "* The future is independent of the past, given the present.\n",
    "* Once the state is know, past history could be thrown away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It provides a simulated environment to test your reinforcement learning algorithms easily with Atari games like Cartpole, Lunar Landing, or Breakout etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the OpenAI Documentation [here](https://gym.openai.com/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# First install some system packages, you can use *homebrew* on macOS\n",
    "brew install cmake boost boost-python sdl2 swig wget\n",
    "\n",
    "# First clone the repo, then install all environments\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e '.[all]'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent-Envrionment Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each timestep, the agent chooses an action, and the environment returns an observation and a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The environment's step function returns four values:\n",
    " - observation (object): an environment-specific object representing your observation of the environment. \n",
    "        For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    " - reward (float): amount of reward achieved by the previous action. \n",
    "    The scale varies between environments, but the goal is always to increase your total reward.\n",
    "    \n",
    " - done (boolean): whether it's time to reset the environment again. \n",
    "    Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. \n",
    "    (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "\n",
    " - info (dict): diagnostic information useful for debugging. \n",
    "    It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart-Pole: [Inverted pendulum problem](https://en.wikipedia.org/wiki/Inverted_pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " - classic problem in dynamics and control theory \n",
    " - widely used as a benchmark for testing control systems\n",
    " - exists analytical solutions based on Newton's Law \n",
    " - we'll be using reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ohWngM-PVYmDG9KVpOm_xQ.gif\", width=50%>\n",
    "<center> Cart-Pole Env from OpenAI Gym </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "At each time step:\n",
    "    \n",
    "Observable States:\n",
    "    - position (cart)\n",
    "    - velocity (cart)\n",
    "    - angle (pole)\n",
    "    - angular velocity (pole)\n",
    "    \n",
    "Possible Actions:\n",
    "    - move left\n",
    "    - move right\n",
    "    \n",
    "State Space ( 4 x 1 continuous)\n",
    "Action Space ( 2 x 1 discrete)\n",
    "\n",
    "Reward:\n",
    "    - pole is still upright (somewhat)\n",
    "    - cart is within the bounding box\n",
    "\n",
    "Done (environment restarts once done == True): \n",
    "    - pole is over a threshold angle\n",
    "    or\n",
    "    - cart is too far off outside the bounding box\n",
    "\n",
    "Solved:\n",
    "    The problem is considered “solved’ when it stays upright for over 195 time steps, 100 times consecutively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We can describe a **policy** by specifying the agent action, given a state.\n",
    "\n",
    "For example, one naive policy would be to just move the cart randomly. \n",
    "This would not be the optimal policy we are searching for, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q matrix:\n",
    "    rows: different states\n",
    "    columns: actions\n",
    "    initialize to be zero\n",
    "    \n",
    "The transition rule of Q learning is a very simple formula:\n",
    "\n",
    "Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI gym for Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just take a random action -- move left or move right at random\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        env.reset()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code snippets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialize the \"Cart-Pole\" environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "## Defining the environment related constants\n",
    "\n",
    "# Number of discrete states (bucket) per state dimension\n",
    "NUM_BUCKETS = (1, 1, 6, 3)  # (x, x', theta, theta')\n",
    "# Number of discrete actions\n",
    "NUM_ACTIONS = env.action_space.n # (left, right)\n",
    "# Bounds for each discrete state\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "STATE_BOUNDS[1] = [-0.5, 0.5]\n",
    "STATE_BOUNDS[3] = [-math.radians(50), math.radians(50)]\n",
    "# Index of the action\n",
    "ACTION_INDEX = len(NUM_BUCKETS)\n",
    "\n",
    "## Creating a Q-Table for each state-action pair\n",
    "q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))\n",
    "\n",
    "## Learning related constants\n",
    "MIN_EXPLORE_RATE = 0.01\n",
    "MIN_LEARNING_RATE = 0.1\n",
    "\n",
    "## Defining the simulation related constants\n",
    "NUM_EPISODES = 1000\n",
    "MAX_T = 250\n",
    "STREAK_TO_END = 120\n",
    "SOLVED_T = 199\n",
    "DEBUG_MODE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate():\n",
    "\n",
    "    ## Instantiating the learning related parameters\n",
    "    learning_rate = get_learning_rate(0)\n",
    "    explore_rate = get_explore_rate(0)\n",
    "    discount_factor = 0.99  # since the world is unchanging\n",
    "\n",
    "    num_streaks = 0\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "\n",
    "        # Reset the environment\n",
    "        obv = env.reset()\n",
    "\n",
    "        # the initial state\n",
    "        state_0 = state_to_bucket(obv)\n",
    "\n",
    "        for t in range(MAX_T):\n",
    "            env.render()\n",
    "\n",
    "            # Select an action\n",
    "            action = select_action(state_0, explore_rate)\n",
    "\n",
    "            # Execute the action\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Observe the result\n",
    "            state = state_to_bucket(obv)\n",
    "\n",
    "            # Update the Q based on the result\n",
    "            best_q = np.amax(q_table[state])\n",
    "            q_table[state_0 + (action,)] += learning_rate*(reward + discount_factor*(best_q) - q_table[state_0 + (action,)])\n",
    "\n",
    "            # Setting up for the next iteration\n",
    "            state_0 = state\n",
    "\n",
    "            # Print data\n",
    "            if (DEBUG_MODE):\n",
    "                print(\"\\nEpisode = %d\" % episode)\n",
    "                print(\"t = %d\" % t)\n",
    "                print(\"Action: %d\" % action)\n",
    "                print(\"State: %s\" % str(state))\n",
    "                print(\"Reward: %f\" % reward)\n",
    "                print(\"Best Q: %f\" % best_q)\n",
    "                print(\"Explore rate: %f\" % explore_rate)\n",
    "                print(\"Learning rate: %f\" % learning_rate)\n",
    "                print(\"Streaks: %d\" % num_streaks)\n",
    "\n",
    "                print(\"\")\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode %d finished after %f time steps\" % (episode, t))\n",
    "                if (t >= SOLVED_T):\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "        learning_rate = get_learning_rate(episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state, explore_rate):\n",
    "    # Select a random action\n",
    "    if random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    # Select the action with the highest q\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_explore_rate(t):\n",
    "    return max(MIN_EXPLORE_RATE, min(1, 1.0 - math.log10((t+1)/25)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_learning_rate(t):\n",
    "    return max(MIN_LEARNING_RATE, min(0.5, 1.0 - math.log10((t+1)/25)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_to_bucket(state):\n",
    "    bucket_indice = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_BUCKETS[i] - 1\n",
    "        else:\n",
    "            # Mapping the state bounds to the bucket array\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            offset = (NUM_BUCKETS[i]-1)*STATE_BOUNDS[i][0]/bound_width\n",
    "            scaling = (NUM_BUCKETS[i]-1)/bound_width\n",
    "            bucket_index = int(round(scaling*state[i] - offset))\n",
    "        bucket_indice.append(bucket_index)\n",
    "    return tuple(bucket_indice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    simulate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
